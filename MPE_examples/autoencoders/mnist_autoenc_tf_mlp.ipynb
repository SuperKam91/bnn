{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Taken/adapted from https://github.com/tensorflow/models/tree/master/research/autoencoder\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#autoencoder class with deep fully connected architecture\n",
    "class Autoencoder(object):\n",
    "\n",
    "    def __init__(self, n_layers, transfer_function=tf.nn.softplus, optimizer=tf.train.AdamOptimizer()):\n",
    "        self.n_layers = n_layers\n",
    "        self.transfer = transfer_function\n",
    "\n",
    "        network_weights = self._initialize_weights()\n",
    "        self.weights = network_weights\n",
    "\n",
    "        # model\n",
    "        self.x = tf.placeholder(tf.float32, [None, self.n_layers[0]])\n",
    "        self.hidden_encode = []\n",
    "        h = self.x\n",
    "        for layer in range(len(self.n_layers)-1):\n",
    "            h = self.transfer(\n",
    "                tf.add(tf.matmul(h, self.weights['encode'][layer]['w']),\n",
    "                       self.weights['encode'][layer]['b']))\n",
    "            self.hidden_encode.append(h)\n",
    "\n",
    "        self.hidden_decode = []\n",
    "        for layer in range(len(self.n_layers)-1):\n",
    "            h = self.transfer(\n",
    "                tf.add(tf.matmul(h, self.weights['decode'][layer]['w']),\n",
    "                       self.weights['decode'][layer]['b']))\n",
    "            self.hidden_decode.append(h)\n",
    "        self.reconstruction = self.hidden_decode[-1]\n",
    "\n",
    "        # cost\n",
    "        self.cost = 0.5 * tf.reduce_sum(tf.pow(tf.subtract(self.reconstruction, self.x), 2.0))\n",
    "        self.optimizer = optimizer.minimize(self.cost)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(init)\n",
    "\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        all_weights = dict()\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "        # Encoding network weights\n",
    "        encoder_weights = []\n",
    "        for layer in range(len(self.n_layers)-1):\n",
    "            w = tf.Variable(\n",
    "                initializer((self.n_layers[layer], self.n_layers[layer + 1]),\n",
    "                            dtype=tf.float32))\n",
    "            b = tf.Variable(\n",
    "                tf.zeros([self.n_layers[layer + 1]], dtype=tf.float32))\n",
    "            encoder_weights.append({'w': w, 'b': b})\n",
    "        # Decoding network weights\n",
    "        decoder_weights = []\n",
    "        for layer in range(len(self.n_layers)-1, 0, -1):\n",
    "            w = tf.Variable(\n",
    "                initializer((self.n_layers[layer], self.n_layers[layer - 1]),\n",
    "                            dtype=tf.float32))\n",
    "            b = tf.Variable(\n",
    "                tf.zeros([self.n_layers[layer - 1]], dtype=tf.float32))\n",
    "            decoder_weights.append({'w': w, 'b': b})\n",
    "        all_weights['encode'] = encoder_weights\n",
    "        all_weights['decode'] = decoder_weights\n",
    "        return all_weights\n",
    "\n",
    "    def partial_fit(self, X):\n",
    "        cost, opt = self.sess.run((self.cost, self.optimizer), feed_dict={self.x: X})\n",
    "        return cost\n",
    "\n",
    "    def calc_total_cost(self, X):\n",
    "        return self.sess.run(self.cost, feed_dict={self.x: X})\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.sess.run(self.hidden_encode[-1], feed_dict={self.x: X})\n",
    "\n",
    "    def generate(self, hidden=None):\n",
    "        if hidden is None:\n",
    "            hidden = np.random.normal(size=self.weights['encode'][-1]['b'])\n",
    "        return self.sess.run(self.reconstruction, feed_dict={self.hidden_encode[-1]: hidden})\n",
    "\n",
    "    def reconstruct(self, X):\n",
    "        return self.sess.run(self.reconstruction, feed_dict={self.x: X})\n",
    "\n",
    "    def getWeights(self):\n",
    "        raise NotImplementedError\n",
    "        return self.sess.run(self.weights)\n",
    "\n",
    "    def getBiases(self):\n",
    "        raise NotImplementedError\n",
    "        return self.sess.run(self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "[{'b': <tf.Variable 'Variable_17:0' shape=(200,) dtype=float32_ref>, 'w': <tf.Variable 'Variable_16:0' shape=(784, 200) dtype=float32_ref>}]\n",
      "[{'b': <tf.Variable 'Variable_19:0' shape=(784,) dtype=float32_ref>, 'w': <tf.Variable 'Variable_18:0' shape=(200, 784) dtype=float32_ref>}]\n",
      "0\n",
      "cost for this batch: 9456.501\n",
      "cost of test batch: 7701.3506\n",
      "cost for this batch: 339.09537\n",
      "cost of test batch: 308.5683\n",
      "cost for this batch: 253.99118\n",
      "cost of test batch: 237.88475\n"
     ]
    }
   ],
   "source": [
    "#def main():\n",
    "# Import data\n",
    "mnist = read_data_sets(\"MNIST_data\", one_hot=False)\n",
    "n = len(mnist.train.images)\n",
    "epochs = 1\n",
    "batch_size = 50\n",
    "iterations = int(n / batch_size * epochs)\n",
    "autoencoder = Autoencoder(n_layers=[784, 200],\n",
    "                          transfer_function = tf.nn.softplus,\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 0.001))\n",
    "for i in range(iterations):\n",
    "    batch = mnist.train.next_batch(batch_size)\n",
    "    cost = autoencoder.partial_fit(batch[0])\n",
    "    if i % 500 == 0:\n",
    "        print \"cost for this batch: \" + str(cost)\n",
    "        print(\"cost of test batch: \" + str(autoencoder.calc_total_cost(mnist.test.next_batch(batch_size)[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
