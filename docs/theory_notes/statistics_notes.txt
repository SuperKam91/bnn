adjoint method for constrained optimisation: suppose we are minimising f(x,p) w.r.t p subject to g(x,p)=0, then we will likely calculate dg/dx. The adjoint method uses these to calculate df/dp (the conjugate transpose of the matrix of dg/dx).
See http://math.mit.edu/~stevenj/18.336/adjoint.pdf, but note that the notation is different to the one used above

sequential tempered mcmc (STMCMC): Runs a series of Markov chains in parallel (and thus can utilise distributed computing). The approach can be split into three parts: annealing, MCMC and importance resampling.
The annealing step introduces intermediate distributions that gradually evolve the samples from the prior to the posterior instead of directly jumping to the posterior. The MCMC step allows the samples to explore the intermediate distributions and adjust to changes as the distributions evolve. The importance resampling step discards unlikely samples and multiplies likely samples to maintain and rebalance the samples with respect to the changes in the intermediate distributions from one level to the next.
https://thesis.library.caltech.edu/10263/1/catanach_thesis_deposit.pdf

implicit sampling: in the context of importance sampling, sample from a distribution which is data-informed, so that it has significant overlap with the likelihood function. To find this informative distribution one finds the mode of the posterior, and one draws from a importance function (e.g. Gaussian) centred on this mode. The variance of the importance function can be calculated from the Hessian matrix at the mode of the original function.
https://math.berkeley.edu/~chorin/TMWC15.pdf

kalman filter: use data (which includes noise) of a process and model describing the process to estimate true value at the next instance. At each step one takes a weighted combination of the observed data and the previous estimated value to give the new estimate.

approximate bayesian computation (ABC): bypasses calculation of likelihood in Bayesian inference. ABC methods approximate the likelihood by simulating the data, which are then compared with real data. Each simulated data is generated using the model and a value of the parameters sampled from the prior. If the real and simulated data are too discrepant the sample is discarded. The measure of discrepancy is usually some metric such as the Euclidean distance. If data is high-dimensional, can be inefficient to simulate, so summary statistics can be compared instead. The smaller the tolerance of the acceptance, the closer the posterior approximation (p[theta | data comparison]) will be to the true posterior samples (p[theta | data]). If the acceptance is too high the posterior approximation will largely resemble the prior.

gaussian process: infinite set of r.v.s for which any finite set have multivariate Gaussian distribution. Can be thought of as a generalisation of a (finite) vector of Gaussian random variables described by a multivariate distribution, to an infinite dimensional vector where the (continuous) domain is spanned by x and the mean and covariance are given by mu(x) and cov(x,y). Samples at each x, z(x) are drawn from Gaussian distributions. In the context of Bayesian inference, one can assign a prior over functions, sample functions from the prior and infer the function describing some data.

kriging: method of interpolation for which the interpolated values are modelled by a Gaussian process. Kriging is similar to regression analysis, but is used when only one realisation of the data is available. Can also be seen as an instance of Bayesian inference- your prior distribution of functions is a Gaussian process, the likelihood is the likelihood of the observed data points for these functions, and your posterior is your updated beliefs.

bayesian regression: assume that the variable you are trying to regress is a random variable with statistics related to the (measured data) independent variables and model parameters (i.e. the likelihood). Bayesian regression then involves determining a probabilistic estimate of your regressed parameter using Bayesian inference (with priors set on the model parameters).

lasso regression: strictly regularised regression (include 'subject to' term in objective function to ensure parameter values are below certain value) i.e. constrained optimisation of cost function.

ridge regression: regularisation to reduce conditioning (sensitivity to inputs) of system.

quantile regression: attempts to estimate median or another quantile of the regression variables, rather than the expected value in least squares. Solved used linear programming instead of linear algebra, and in particular by using the simplex method. 

active subspaces: identify dimensions in which scalar valued function changes the most by inspecting gradient of function w.r.t its variables. Active subspace is formed of linear combinations of original dimensions. Directions orthogonal to active subspace are known as inactive subspace. Can be used as a dimensionality reduction tool. Can be used to approximate original function as follows. First determine the active subspace, and sample along its range. Next for each sample in the active subspace, sample the function along the inactive subspace. Finally, take the conditional average of the samples in the inactive subspace (given a point in the active subspace). Differs from PCA in the sense that PCA works on a vector of data (and transforms to a basis where each dimension is uncorrelated).

Bayesian model averaging (BMA): Bayesian model selection usually involves picking the 'best' model by comparing Bayes factors. In BMA if some model-independent quantity y is to be obtained from considering various models M_i and data x, then it is given by the weighted average p[y | d] = sum_i p[M_i | d] p[y | M_i, d] thus each p[y | M_i, d] is weighted by the probability of the model (given the data).

multi-level monte carlo: e.g. two-level Monte Carlo, if we want to estimate E[f_1] but E[f_0] is much cheaper to calculate we can say E[f_1] = E[f_0] + E[f_1 - f_0] = 1 / n_0 sum_n^n_0 f_0^[n] + 1 / n_1 sum_n^n_1 f_1^[n] - f_0^[n]. Trivial example, f_1 is double precision calculations, f_0 is single precision calculations. Trade off between more calculations of f_0 (more uncertainty in estimate) and more calculations of f_1 (more costly). Generalises to l-levels. In calculating f_0, one loses by introducing bias in estimate (as it is a less accurate estimate of f) but gains in the cost of each calculation. In calculating f_0, one loses by cost of each computation, but each calculation should be less biased. Aim is to calculate f_0 and f_1 a number of times such that the bias of the estimate is reduced (by calculating f_1), and the 'usual' variance associated with Monte Carlo estimates (which goes as 1/n) is reduced (by calculating f_0).

multi-index multi-level monte carlo: generalisation of multi-level Monte Carlo, in which multi-level Monte Carlo is considered in more than one dimension of f. This allows for multi-levelling to take place between mixes of the dimensions as well as conducting it on each separately. 

laplace approximation: Taylor expand logarithm of probability distribution around its peak (up to second order, first derivative is zero), so that the probability distribution can be approximated by a Gaussian distribution, and its normalisation constant can be approximated by the constant of the Gaussian. 

bernstein von-mises theorem: as the number of data provided increases asymptotically, the choice of prior distribution becomes irrelevant.

'almost surely' / 'almost everywhere': one says that an event happens if it happens with probability one. In other words, the set of possible exceptions may be non-empty, but it has probability zero.

latin lattice sampling: Sampling such that only one sample occurs in each row and column of the grid. When sampling a function of N variables, the range of each variable is divided into M equally probable intervals. M sample points are then placed to satisfy the Latin hypercube requirements; note that this forces the number of divisions, M, to be equal for each variable. 

bayesian quadrature: Can treat value of integrand as random variable, and thus function you are integrating as random variable. Use a Gaussian process as prior over functions, access likelihood with real function sampled at points determined by some process to get posterior distribution over functions. Take mean of posterior and calculate its integral for mean estimate of the integral. Values of integrals of other functions (weighted by posterior) give variance in integral estimate.

information maximising neural networks: reduce dimensionality of parameter space by training a neural network based on maximising the fisher information of the output variables (use the fisher information of the input [original] variables as a proxy for the maximum fisher values), where output layer is smaller than input layer. Do this for simulated data, and then the NN can be applied to the real data and the likelihood evaluated in this reduced-dimension space.

hamiltonian monte carlo: Total Hamiltonian H is constant w.r.t time: H(p, q) = T(p) + V(q) which leads to two first order ODEs: one in q and T and one in p and V which can be solved (given initial conditions for p and q) to determine p and q at a later time. In HMC, probability distribution is given by exp(-H), where V = - log(target distribution[q])  (here position refers to parameter values of target distribution), momentum part is picked arbitrarily to provide 'acceleration' to system, but must be included to use Hamiltonian equations (ODE in position depends on derivative of T w.r.t momentum). So once position has been set (previous step position. At initialisation of algorithm this is set randomly) and momentum randomly generated, system is evolved, and acceptance should be ~100% since energy is constant (acceptance ratio, which is ratio of energies, should be ~1). Note energy isn't completely constant due to discretisation of time. Due to way we defined V, momentum will move chain in direction of increasing posterior. Note at posterior modes, chain will not be stationary as randomness of momentum will cause it to move around.

langevin monte carlo (MALA): consists of two steps, first find proposed state using (overdamped) Langevin dynamics, the proposed state is then accepted or rejected based on the Metropolis acceptance ratio. Like HMC, uses gradient of the target distribution to evolve the dynamics of the system. The dynamics of the system are described by the Langevin Ito diffusion equation, which includes a Brownian motion term (its time derivative) as well as the gradient (of the proposal distribution).

sequential monte carlo (particle filtering): For set of observables {y_i} and set of parameters of interest {x_i}, both indexed by time i = 0, ..., t, sequential Monte Carlo can be used to determine posterior distribution p[x_0, ..., x_t | y_0, ..., y_t]. Furthermore, if y_t+1 is observed, updated posterior p[x_0, ..., x_t+1 | y_0, ..., y_t+1] can be obtained without resampling previous states. Note that in SMC there are two indexes, one in time (t), and one in the number of samples/trajectories at a particular timestep (n). The x_t are simulated from an importance distribution whose x_t'th sample does not require the previous x_t-1 timesteps to be re-simulated: pi[x_0, ..., x_t+1 | y_0, ..., y_t+1] = pi[x_0, ..., x_t | y_0, ..., y_t] pi[x_t+1 | x_0, ..., x_t, y_0, ..., y_t+1] (which implies that pi[x_0, ..., x_t | y_0, ..., y_t] = pi[x_0, ..., x_t | y_0, ..., y_t+1] from the product rule, but I guess this makes sense as surely y_t+1 doesn't affect x_t, and I think we have assumed that x_t depends on x_t-1 only), from which the importance weights can also be defined recursively. However, these weights become exponentially dominated by one of the n x_t as t increases, and so another step must be introduced to make the weights more even. One example of this procedure is the bootstrap filter, which eliminates samples with small weights and multiples ones with high weights such that there are still n particles. The new weights of the samples are then integers given by the number of offspring associated with each sample, and the number of offspring is determined by the old weights (the integer weights are obtained by sampling the discrete old weight distribution). Algorithm  works in following steps: first initialises the n x_0 by simulating from importance distribution, then in the main loop: samples x_1 from new importance distribution and assigns them weights. Next the integer weights are calculated from the importance weights, the algorithm then repeats for x_2. 

piecewise deterministic monte carlo: Treat Markov chain as sequence of tuples (x_i, v_i) where x_i denotes position (parameter value) and v_i is the velocity. Between time steps x_i is calculated deterministically depending on as x_i+1 = x_i + v_i * tau where tau is the timestep. The time steps are simulated from n inhomogeneous Poisson processes which depend on x_i and v_i. The distribution from which the new v_i+1 is sampled from depends on which Poisson process was used to determine the timestep. Actual timestep considered is smallest of the n sampled, and this sample is accepted or rejected based on ratio of function dependent on current position, velocity and timestep over a function of current timestep. Upon rejection, position is still updated as normal, but v_i+1 is set to v_i. Upon acceptance v_i+1 is resampled.

quasi monte carlo: uses low discrepancy sequences (a sequence is said to have a low discrepancy if the proportion of the sequence in some set is roughly proportional to the measure over the set) of points rather than pseudorandom sequences in Monte Carlo. QMC methods are considered deterministic, but can be made 'more random' by introduced random shifts to the sequence.

random field: stochastic process such that underlying parameter is a vector of values or points on a manifold (as opposed to a single number index as is the case for normal random variables).

stein's method: can be used to find upper bound on metric such as Wasserstein distance between two probability distributions p and q which take the form sup{h isin H E_p[h(x)] - E_q[h(x)]} where H is some set of test functions. If p is some target distribution and q is the sampled distribution, the overhead of calculating such a metric can be reduced using Stein's method, as it generates an upperbound on the metric as an expectation over q only (which is generally much easier to evaluate than the 'true' distribution p). This requires finding an operator t and a set of functions G such that E_q[tg] = 0 for all g isin G iff p = q. The upper bound is then sup{g isin G E_q[tg]}.

confidence interval: for a given confidence level, the interval in which the true value of the parameter will lay if the value of the parameter is estimated an infinite number of times. Can be calculated around mode of distribution, since in this region the Taylor expansion of the distribution is Gaussian to second order (in which case the mean = mode). Thus with the Gaussian approximation the confidence interval will be symmetric about the mode/mean. In higher dimensions Gaussian may be elliptical. Can calculate asymmetric confidence intervals by not using Gaussian approximation. Multi-dimensional generalisation is called confidence region.

credible interval: range of values within which an unobserved parameter value falls with a particular subjective probability. Credible intervals are analogous to confidence intervals in frequentist statistics, although they differ on a philosophical basis; Bayesian intervals treat their bounds as fixed and the estimated parameter as a random variable, whereas frequentist confidence intervals treat their bounds as random variables and the parameter as a fixed value. Choosing the narrowest interval, which for a unimodal distribution will involve choosing those values of highest probability density including the mode, is called the highest posterior density interval. The generalisation to multivariate problems is the credible region.

bayesian vs frequentist thinking: frequentists see data (experiment) as being repeated many times and thus subject to variability, the (population) parameters are known and not uncertain. The Bayesian way of thinking is quite the contrary: the parameter is considered uncertain, while the data is measured once. As a consequence, in frequentist statistics when considering the likelihood function, the data is considered stochastic, while in Bayesian terms it is the parameters which are stochastic. Similarly, a confidence interval in frequentist terms is the interval in which if the data are sampled many times, the parameter will lay in this interval a given percentage of the time. A Bayesian credible interval gives a degree of belief that the parameter value lays in the specified interval.

bayesian hierarchical modelling: where you have a hierarchy of random variables. e.g. in a typical Bayesian inference problem, you would have a prior pr[theta|model1]. If the hyperparameters of pr[theta|model1] are also random variables, then you need to assign probability distributions to them i.e. pr[hyperparams|model2]. Another example is linear regression y = mx + c. m and c could have associated distributions pr[m|model1] and pr[c|model2], and the hyperparameters of these distributions could also be random variables pr[hyperparams1|model3] and pr[hyperparams2|model4]. Factorisation (product rule) of hierarchy can result in posteriors being estimated by joint distribution on data and parameters, and marginalisation.

non-reversible markov chains: reversibility is a sufficient but not necessary condition for a Markov chain to have the target distribution as its invariant distribution in MCMC. In the case of non-reversible Markov chains, still have to satisfy necessary conditions for chain to converge on target distribution. Non-reverisible element of Markov chain is often deterministic, and governed by e.g. non-reversible differential equations. n.b. Hamiltonian has non-reversible characteristics, but apparently, is ultimately reversible due to the accept/reject step.

adapative mcmc: Change nature of Markov chain in some way as the chain is run, no longer strictly Markovian, so care must be taken to ensure chain converges to target distribution. One common example is changing the variance of the proposal distribution to ensure the acceptance rate stays at the desired value (0.234 is a common aim for certain target distributions).

zig-zag mcmc: non-reversible mcmc method where trajectory is governed by differential equation in time. The trajectory then switches direction (changes sign) according to a probability related to gradient of target.

dirchlet distribution: multi-variate generalisation of beta distribution, seems to be product of polynomials.

perfect separation (classification): one of the independent variables can be used to perfectly classify the training data. Means MLE doesn't have unique solution, as regression coefficient associated with perfect estimator goes to infinity.

variational autoencoder: learn vectors of mean and standard deviation, which are sampled to produce encoding. more robust to noise in data than vanilla autoencoders, I believe.

gan: teach two neural networks simultaneously, with goal of one trying to generate data similar to some input, and the other being able to discriminate between the real data and the generated data.

semi-supervised learning: have input data with known labels, and input data with unknown labels. idea is to train nn that works on both of these types of data. generally have two components to loss function, one part classifying data with labels, and one part evaluating unlabelled data. for example with a semi-supervised gan, you use discrminator to discrminiate between real unlabelled data and generated data, while classifying the labelled data to one of the classes. for a k-classification the discriminator therefore classifies to k+1 labels: the k classes, or whether the data is fake. 
