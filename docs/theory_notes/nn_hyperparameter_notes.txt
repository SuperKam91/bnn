- if using regularisation, don't think running non-Bayesian (NB) methods for a large number of epochs should lead to overfitting training data. Just because algo is looking for minimum of cost function, doesn't mean this is attributed to increasing likelihood, could be equally applicable to increasing prior value, which generally, will reduce overfitting.

- similarly, I don't think running Bayesian methods (B) for a long time (i.e. ensuring the estimate of the remaining evidence is small compared to the currently accumulated value) will lead to overfitting the training data when using the MPE estimate. Note it probably won't increase the accuracy of the MPE estimate, unless the weights are binned (though this isn't really a maximum estimate). It will however, give a more accurate value of the MLE (PC outputs the LL of the samples, as well as the posterior weights). It will also enhance the accuracy of the expected prediction to a small extent, but this shouldn't cause it to overfit. running PC for a long time may prevent some modes being missed, which would have a large effect on the expected estimate (EE), and possibly the MPE/MLE predictions.

- running PC with a high number of live points has effects similar to above, but more pronounced. though I suspect it will give more accurate estimates of the MPE, the MLE, and the EE. Increasing it will also greatly enhance the chances of capturing more posterior modes. It will also increase the accuracy of the evidence estimate.

- hyperparameters common to both NB and B methods include:
	number of layers
	number of nodes in each layer
	activation functions for each layer/node
	regularisation coefficients (c.f. likelihood and prior variances)
	batch size

- NB methods have a number of other hyperparameters, including:	
	learning rates (and their decay rates)
	parameters associated with gradient descent method (e.g. momentum parameters, number of steps, stepsize)

- B methods i.e. PolyChord have other hyperparameters:
	number of slices per sample
	how often to re-do contour whitening (though I don't think this is directly adjustable)

- in NB methods, can't train regularisation coefficients on training data, as their maximum value is zero. Hence cross validation is usually used- nn is trained on training data with given regularisation coeffs, then the performance is evaluated on a cross validation set. whichever coeffs minimise the cost on the cross validation set are selected.

- in B methods, regularisation coefficients are treated as stochastic during training. Mackay uses some form of MLE estimations of the coefficients as the final values, while Neal marginalises over them when making predictions on test data. similar to NB methods, using a uniform prior over the coeffs would result in low values being favoured regardless of the data. one of the priors on the coeffs Neal uses is the gamma distribution, which results in the posterior not having a maximum at coeffs = zero. this gamma prior on the regularisation coeff (prior on prior hyperparam) could just as well be added to the cost function in the NM methods, meaning the minimum of the cost is no longer at coeffs = 0. this could potentially save the cross validation technique. however, I don't think I've ever seen this in the literature. using this would correspond to the MPE estimate of the B method.

- n.b. gamma prior on regularisation coefficients is common because it is conjugate with a gaussian prior on the weights, where the mean of the gaussian is known, and its precision (1/variance i.e. the regularisation coefficient) is the stochastic parameter of the gamma prior.

- apparently, dropout regularisation has been shown to be better than vanilla regularisation in this paper: https://arxiv.org/abs/1307.1493?context=stat.ME

