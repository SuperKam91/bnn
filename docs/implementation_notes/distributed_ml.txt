- Two main paradigms for distributed machine learning. Either share same complete model over nodes and split data, or share same data and split model across the nodes. The former is more popular in practice.

- Model parallelism can be useful if e.g. each layer is split across computation nodes (i.e. each computation node gets one or more nodes in a given layer), as these computations can generally be done independently.

- If model is parallelised say by layer, hard to see how this can provide much of a speedup, as one node has to wait for nodes computing previous nodes to finish.

- Data parallelism relies on each node computing the forward and backward propagation independently of the other nodes, and combining the results (parameter updates) in some way.

- The updates to parameters can be synchronous, meaning each node has their model parameters updated simultaneously, or asynchronous where each node's parameters are updated out of sync.

- Methods can also be centralised or decentralised. In the former case one usually has a central node which takes care of the global updating of the model parameters. In the latter case the compute nodes communicate directly with one another.

- An example of a synchronous method is parameter averaging, where each node computes the forward and backward propagation, updates its model parameters then sends the updated parameters back to a central node. At the central node the average of these updated parameters is calculated and considered the 'new' update value, before being passed back to the worker nodes for them to calculate another pass with their respective data.

- Can also be made asynchronous by updating as soon as it gets a few updates, and sending these out to whichever nodes need to do another pass.

- Instead of averaging parameters, can just send deltas in parameter values to central node, which can then be used to asynchronously update the parameters. If updates are 'out of date' can weight them down accordingly. In an asynchronous context, makes more sense to transfer deltas I think.

- Decentralised methods often involve compressing updates in some way (i.e. only using parts of them), to speed up transfer process.

- Ensemble learning can be done easily in a distributed context: separate models are trained on each node, and final predictions are some linear combination of each individual model's outputs (or vote). 

- If doing a gridsearch, distributed systems are great as each NN can be trained independently (i.e. each node trains a NN on entire data).
