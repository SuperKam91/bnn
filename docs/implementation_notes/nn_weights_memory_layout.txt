- for now assume x has shape (m, n_inp) and we want y to have shape (m, n_out). can mess around with these quite easily, but how the training data is read from file may have to be adapted for both python and c++, and change how they are stored.

- for either c++, np, tf or k implementations, polychord will pass a 1-d array of parameters to the lhood functions.

- by default numpy stores arrays row-wise, but eigen stores them column-wise.

- for a given weight array 1w, these can either be in the order w_11, w_21, ..., 1w_i1, 1w_12, w_22, ..., w_ij or w_11, w_12, ..., w_1j, w_21, w_22, ..., w_ij where w_mn denotes the element (m,n) of the weight matrix w. Denote these orderings 1wc and 1wr respectively.

- for the c++ implementation, if we evaluate x * w, then I believe it is more efficient to have x stored row-wise and w stored column-wise, in which case we want the ordering to be 1wc. 

- alternatively we can have the 1wc ordering and store w row-wise, but I think this is less efficient.

- if instead we evaluate (w^t * x^t)^t, then for efficiency we should store x column-wise and w row-wise, but I think all the transposing will add a dominating overhead (especially since x and first w matrix are maps). 

- should check if transposing map somehow does it inplace, if it does then it won't reference adjacent matrix elements contigously in memory. if it doesn't then it must make a copy. both add overhead

- in python implementation, when reading 1w, can store values in w matrices in column-wise order by changing the following line in get_np_weights/get_k_weights/get_tf_weights:
weights.append(new_oned_weights[start_index:start_index + weight_size].reshape(weight_shape)) -> weights.append(new_oned_weights[start_index:start_index + weight_size].reshape(weight_shape, order='F'))
but NOTE the documentation says that the underlying storage cannot be guaranteed to be column-wise. only way to get over this is to wrap reshaped_array as np.array(reshaped_array, order='F'). though I'm not 100% sure even this rectifies the issue.

- NOTE, operating on a row-major and a column-major matrix which results in a matrix being returned, leads to the returned matrix being in row-major format (this affects activation matrices). again, I think this can be alleviated by doing np.array(col_row_matrices_op_returned_array, order='F'), but note this may change ordering of matrix elements

- however x_tr and y_tr are retrieved, they can be stored in column-wise order.

- messing around with storage order of training data is less crucial, as it should be a one-time operation at initiation of the program.

- in tf and keras, seems pretty difficult to change to colmajor matrices.

- for now I will use row-major for everything apart from final output layer of c++ nn, even though this may be sub-optimal for certain areas

- note that in traditional nn diagrams, the weight matrices have shapes (layer_size, prev_layer_size), as this suits the left-to-right diagrams where each node in a layer is stacked vertically (and each node contains a linear combination of the previous nodes, displayed horizontally).
In this work the weights have shape (prev_layer_size, layer_size). There's no particular reason I did this, but I don't think it benefits from changing it. Thus when labelling the parameters of the inference, for a given layer, w_jk denotes the weight multiplying the output of the jth node from the previous layer in the kth node of the current layer. Hence for layer i, w_ijk denotes this element of the matrix for the ith layer. Similarly, b_ij represents the jth element of the bias vector in the ith layer.
This helps with the priors on the weights, as the w_ij1,  w_ij2, ..., w_ijlayer_size_of_layer_i random variables (which are contiguous in memory with this notation and using row-major storage) are dependent on one another (to prevent layer degeneracy). The same applies for the b_i1, b_i2, b_ilayer_size_of_layer_i random variables