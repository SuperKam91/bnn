{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "Model = tf.keras.models.Model\n",
    "Input = tf.keras.layers.Input\n",
    "Dense = tf.keras.layers.Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 2\n",
    "num_outputs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slp_model():\n",
    "    \"\"\"\n",
    "    keras model builder (using model api) for single layer perceptron classification nn\n",
    "    \"\"\"\n",
    "    a0 = Input(shape = (num_inputs,))\n",
    "    a1 = Dense(5, activation = 'relu')(a0)\n",
    "    prediction = Dense(num_outputs, activation='linear')(a1)\n",
    "    return Model(inputs = a0, outputs = prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = slp_model()\n",
    "model.compile(loss='mse', optimizer='rmsprop')\n",
    "np.random.seed(1337)\n",
    "x_tr = np.random.random((7,2))\n",
    "y_tr = x_tr**2.\n",
    "km = keras_model(model, x_tr, y_tr, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.setup_LL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.26202468 0.15868397]\n",
      " [0.27812652 0.45931689]\n",
      " [0.32100054 0.51839282]\n",
      " [0.26194293 0.97608528]\n",
      " [0.73281455 0.11527423]\n",
      " [0.38627507 0.62850118]\n",
      " [0.12505793 0.98354861]]\n",
      "y\n",
      "[[0.06865693 0.0251806 ]\n",
      " [0.07735436 0.210972  ]\n",
      " [0.10304135 0.26873112]\n",
      " [0.0686141  0.95274248]\n",
      " [0.53701717 0.01328815]\n",
      " [0.14920843 0.39501373]\n",
      " [0.01563948 0.96736786]]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print km.x_tr \n",
    "print 'y'\n",
    "print km.y_tr\n",
    "print km.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.73281455 0.11527423]\n",
      " [0.32100054 0.51839282]]\n",
      "[[0.26202468 0.15868397]\n",
      " [0.38627507 0.62850118]]\n",
      "[[0.27812652 0.45931689]\n",
      " [0.12505793 0.98354861]]\n",
      "[[0.26194293 0.97608528]]\n",
      "y\n",
      "[[0.53701717 0.01328815]\n",
      " [0.10304135 0.26873112]]\n",
      "[[0.06865693 0.0251806 ]\n",
      " [0.14920843 0.39501373]]\n",
      "[[0.07735436 0.210972  ]\n",
      " [0.01563948 0.96736786]]\n",
      "[[0.0686141  0.95274248]]\n"
     ]
    }
   ],
   "source": [
    "x_batch_1, y_batch_1 = km.get_batch()\n",
    "x_batch_2, y_batch_2 = km.get_batch()\n",
    "x_batch_3, y_batch_3 = km.get_batch()\n",
    "x_batch_4, y_batch_4 = km.get_batch()\n",
    "print x_batch_1\n",
    "print x_batch_2\n",
    "print x_batch_3\n",
    "print x_batch_4\n",
    "print 'y'\n",
    "print y_batch_1\n",
    "print y_batch_2\n",
    "print y_batch_3\n",
    "print y_batch_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class keras_model():\n",
    "    \"\"\"\n",
    "    class which includes keras model, intended for forward propagation of nn only.\n",
    "    will eventually contain method that calculates likelihood associated with nn, input and output data.\n",
    "    note that it assumes model has already been compiled (keras.model.compile(...)) so that loss function is defined.\n",
    "    steps:\n",
    "    1) create instance of keras.Model, compile it\n",
    "    2) create instance of keras_model\n",
    "    3) setup likelihood function by calling get_LL_const\n",
    "    4) pass class to polychord function, along with additional arguments (NEED TO BE CONFIRMED)\n",
    "    --additional notes--\n",
    "    technically, self.weights isn't really needed at all, just there for testing purposes atm.\n",
    "    same goes for self.oned_weights. \n",
    "    initially thought of keeping nn initialised weights for first iteration, \n",
    "    but this would only give values for one livepoint, so should use pc initialisation to initialise weights.\n",
    "    num_weights also probably not necessary.\n",
    "    \"\"\"\n",
    "    def __init__(self, k_model, x_tr, y_tr, batch_size):\n",
    "        \"\"\"\n",
    "        assign model to class, calculate shape of weights, and arrays containing them (latter possibly redundant)\n",
    "        \"\"\"\n",
    "        self.weight_shapes = []\n",
    "        self.weights = [] #delete after testing\n",
    "        self.num_weights = 0 #delete after testing\n",
    "        self.oned_weights = np.array([]) #possibly delete after testing\n",
    "        self.model = k_model\n",
    "        self.x_tr = x_tr\n",
    "        self.y_tr = y_tr\n",
    "        self.m = x_tr.shape[0]                  \n",
    "        self.batch_size = batch_size\n",
    "        self.num_complete_batches = int(np.floor(float(self.m)/self.batch_size))\n",
    "        self.num_batches = int(np.ceil(float(self.m)/self.batch_size))\n",
    "        self.get_weight_info()\n",
    "        \n",
    "    def calc_gauss_LL(self, x, y, LL_var = 1.):\n",
    "        \"\"\"\n",
    "        WARNING: batch size given here should be same as one given in get_LL_const, or \n",
    "        normalisation constant won't be correct.\n",
    "        as above, only supports scalar variance.\n",
    "        \"\"\"\n",
    "        return - 1. / (2. * LL_var) * self.model.evaluate(x, y) + self.LL_const \n",
    "        \n",
    "    def setup_LL(self, LL_var = 1.):\n",
    "        \"\"\"\n",
    "        calculates LL constant, and sets correct LL function.\n",
    "        currently only supports single (scalar) variance across all records and outputs,\n",
    "        since we use model.evaluate() to calculate cost with predefined loss functions (e.g. mse, crossentropy) \n",
    "        which evaluates sums across examples/outputs\n",
    "        (so variance can't be included in each summation).\n",
    "        if we instead calculate likelihood from final layer output, will probably use scipy.stats to do so\n",
    "        and this function will become redundant. \n",
    "        if instead we define own loss function which allows for different variances,\n",
    "        comment out lines below (and variance should be LL_dim x LL_dim array) and uncomment ones further down\n",
    "        \"\"\"\n",
    "        self.batch_generator = self.create_batch_generator()\n",
    "        output_size = np.prod(np.shape(model.layers[-1].output.shape)) #np.prod is in case output isn't vector\n",
    "        LL_dim = self.batch_size * output_size\n",
    "        if self.model.loss == 'mse':\n",
    "            #temporary\n",
    "            self.LL_const = -0.5 * LL_dim * (np.log(2. * np.pi) + np.log(LL_var))\n",
    "            self.LL = self.calc_gauss_LL\n",
    "            #longer term solution (see comments above)\n",
    "            #self.LL_const = -0.5 * (LL_dim * np.log(2. * np.pi) + np.log(np.linalg.det(variance)))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "    def get_weight_info(self):\n",
    "        \"\"\"\n",
    "        weight arrays may be redundant (see above)\n",
    "        \"\"\"\n",
    "        for layer_weight in self.model.get_weights():\n",
    "            layer_shape = layer_weight.shape\n",
    "            self.weight_shapes.append(layer_shape)\n",
    "            self.weights.append(layer_weight) #delete after testing\n",
    "            self.oned_weights = np.concatenate((self.oned_weights, layer_weight.reshape(-1))) #possibly delete after testing\n",
    "            self.num_weights += np.prod(layer_shape) #delete after testing\n",
    "        \n",
    "    def get_weight_shapes(self):\n",
    "        return self.weight_shapes\n",
    "    \n",
    "    def get_weights(self): #delete after testing\n",
    "        return self.weights\n",
    "    \n",
    "    def get_oned_weights(self): #possibly delete after testing\n",
    "        return self.oned_weights\n",
    "    \n",
    "    def get_oned_weights2(self): #possibly delete after testing\n",
    "        \"\"\"\n",
    "        used in case where we want array of nn initial weights to pass to pc, but after that\n",
    "        it can be deleted.\n",
    "        WARNING: deletes self.oned_weights\n",
    "        \"\"\"\n",
    "        temp = self.oned_weights\n",
    "        del self.oned_weights\n",
    "        return temp\n",
    "    \n",
    "    def get_num_weights(self): #delete after testing\n",
    "        return self.num_weights\n",
    "    \n",
    "    def get_model(self):\n",
    "        \"\"\"\n",
    "        return keras model instance\n",
    "        \"\"\"\n",
    "        return self.model\n",
    "    \n",
    "    def get_model_summary(self):\n",
    "        \"\"\"\n",
    "        return keras model summary\n",
    "        \"\"\"\n",
    "        return self.model.summary()\n",
    "    \n",
    "    def get_model_weights(self):\n",
    "        \"\"\"\n",
    "        returns list of weight arrays (one element for each layer's set of weight/bias)\n",
    "        \"\"\"\n",
    "        return self.model.get_weights()\n",
    "    \n",
    "    def set_weights(self, weights): #delete after testing\n",
    "        self.weights = weights\n",
    "        \n",
    "    def set_oned_weights(self, oned_weights): #possibly delete after testing\n",
    "        self.oned_weights = oned_weights\n",
    "        \n",
    "    def set_model_weights(self, weights):\n",
    "        self.model.set_weights(weights)\n",
    "        \n",
    "    def set_k_weights(self, new_oned_weights):\n",
    "        self.set_oned_weights(new_oned_weights) #possibly delete after testing\n",
    "        new_weights = []\n",
    "        start_index = 0\n",
    "        for weight_shape in self.get_weight_shapes():\n",
    "            weight_size = np.prod(weight_shape)\n",
    "            new_weights.append(new_oned_weights[start_index:start_index + weight_size].reshape(weight_shape))\n",
    "            start_index += weight_size\n",
    "        self.set_weights(new_weights) #delete after testing\n",
    "        self.set_model_weights(new_weights)\n",
    "        \n",
    "    def __call__(self, oned_weights):\n",
    "        self.set_k_weights(oned_weights)\n",
    "        x_batch, y_batch = self.get_batch()\n",
    "        LL = self.LL(x_batch, y_batch)\n",
    "        return LL\n",
    "        \n",
    "    def get_batch(self):\n",
    "        if self.m <= self.batch_size:\n",
    "            return self.x_tr, self.y_tr\n",
    "        else:\n",
    "            return self.batch_generator.next()\n",
    "            \n",
    "    def create_batch_generator(self):\n",
    "        i = 0\n",
    "        batches = self.create_batches()\n",
    "        while True:\n",
    "            if i < self.num_batches:\n",
    "                pass #don't need to create new random shuffle of training data\n",
    "            else:\n",
    "                batches = self.create_batches()\n",
    "                i = 0\n",
    "            yield batches[i]\n",
    "            i += 1\n",
    "    \n",
    "    def create_batches(self):\n",
    "        \"\"\"\n",
    "        create batches of size self.batch_size from self.m training examples\n",
    "        for training.\n",
    "        In case of large training sets, batches may want to overwrite self.x_tr/self.y_tr\n",
    "        (if not both have to be saved to memory as batches is saved in generator object)\n",
    "        \"\"\"\n",
    "        batches = []\n",
    "        # Step 1: Shuffle x, y\n",
    "        permutation = np.random.permutation(self.m)\n",
    "        shuffled_x = self.x_tr[permutation]\n",
    "        shuffled_y = self.y_tr[permutation]\n",
    "        # Step 2: Partition (shuffled_x, shuffled_y). Minus the end case.\n",
    "        # number of batches of size self.batch_size in your partitionning\n",
    "        for i in range(self.num_complete_batches):\n",
    "            batch_x = shuffled_x[self.batch_size * i: self.batch_size * (i + 1)]\n",
    "            batch_y = shuffled_y[self.batch_size * i: self.batch_size * (i + 1)]\n",
    "            batch = (batch_x, batch_y)\n",
    "            batches.append(batch)\n",
    "    \n",
    "        # Handling the end case (last batch < self.batch_size)\n",
    "        if self.num_complete_batches != self.num_batches:\n",
    "            batch_x = shuffled_x[self.num_complete_batches * self.batch_size:]\n",
    "            batch_y = shuffled_y[self.num_complete_batches * self.batch_size:]\n",
    "            batch = (batch_x, batch_y)\n",
    "            batches.append(batch)\n",
    "        return batches\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "keras_model instance has no __call__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-ca51c3f408f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: keras_model instance has no __call__ method"
     ]
    }
   ],
   "source": [
    "km()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
