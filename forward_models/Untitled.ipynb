{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "Model = tf.keras.models.Model\n",
    "Input = tf.keras.layers.Input\n",
    "Dense = tf.keras.layers.Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 2\n",
    "m = 2\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slp_model(num_inputs):\n",
    "    \"\"\"\n",
    "    keras model builder (using model api) for single layer perceptron classification nn\n",
    "    \"\"\"\n",
    "    a0 = Input(shape = (num_inputs,))\n",
    "    a1 = Dense(5, activation = 'relu')(a0)\n",
    "    prediction = Dense(num_outputs, activation='linear')(a1)\n",
    "    return Model(inputs = a0, outputs = prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = slp_model()\n",
    "model.compile(loss='mse', optimizer='rmsprop')\n",
    "np.random.seed(1337)\n",
    "x_tr = np.random.random((m,num_inputs))\n",
    "y_tr = x_tr**2.\n",
    "km = keras_model(model, x_tr, y_tr, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================]2/2 [==============================] - 0s 3ms/step\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-3.6757541328186907"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.setup_LL()\n",
    "km(np.zeros(27))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]], dtype=float32),\n",
       " array([0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.get_model_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.26202468 0.15868397]\n",
      " [0.27812652 0.45931689]]\n",
      "y\n",
      "[[0.06865693 0.0251806 ]\n",
      " [0.07735436 0.210972  ]]\n"
     ]
    }
   ],
   "source": [
    "print km.x_tr \n",
    "print 'y'\n",
    "print km.y_tr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.26202468 0.15868397]\n",
      " [0.27812652 0.45931689]]\n",
      "[[0.26202468 0.15868397]\n",
      " [0.27812652 0.45931689]]\n",
      "[[0.26202468 0.15868397]\n",
      " [0.27812652 0.45931689]]\n",
      "[[0.26202468 0.15868397]\n",
      " [0.27812652 0.45931689]]\n",
      "y\n",
      "[[0.06865693 0.0251806 ]\n",
      " [0.07735436 0.210972  ]]\n",
      "[[0.06865693 0.0251806 ]\n",
      " [0.07735436 0.210972  ]]\n",
      "[[0.06865693 0.0251806 ]\n",
      " [0.07735436 0.210972  ]]\n",
      "[[0.06865693 0.0251806 ]\n",
      " [0.07735436 0.210972  ]]\n"
     ]
    }
   ],
   "source": [
    "x_batch_1, y_batch_1 = km.get_batch()\n",
    "x_batch_2, y_batch_2 = km.get_batch()\n",
    "x_batch_3, y_batch_3 = km.get_batch()\n",
    "x_batch_4, y_batch_4 = km.get_batch()\n",
    "print x_batch_1\n",
    "print x_batch_2\n",
    "print x_batch_3\n",
    "print x_batch_4\n",
    "print 'y'\n",
    "print y_batch_1\n",
    "print y_batch_2\n",
    "print y_batch_3\n",
    "print y_batch_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' with argument inside generator (<ipython-input-206-1bfbadcdfc00>, line 162)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-206-1bfbadcdfc00>\"\u001b[0;36m, line \u001b[0;32m162\u001b[0m\n\u001b[0;31m    yield batches[i]\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' with argument inside generator\n"
     ]
    }
   ],
   "source": [
    "class keras_model():\n",
    "    \"\"\"\n",
    "    class which includes keras model, intended for forward propagation of nn only.\n",
    "    will eventually contain method that calculates likelihood associated with nn, input and output data.\n",
    "    note that it assumes model has already been compiled (keras.model.compile(...)) so that loss function is defined.\n",
    "    steps:\n",
    "    1) create instance of keras.Model, compile it\n",
    "    2) create instance of keras_model\n",
    "    3) setup likelihood function by calling get_LL_const\n",
    "    4) pass class to polychord function, along with additional arguments (NEED TO BE CONFIRMED)\n",
    "    --additional notes--\n",
    "    technically, self.weights isn't really needed at all, just there for testing purposes atm.\n",
    "    same goes for self.oned_weights. \n",
    "    initially thought of keeping nn initialised weights for first iteration, \n",
    "    but this would only give values for one livepoint, so should use pc initialisation to initialise weights.\n",
    "    num_weights also probably not necessary.\n",
    "    \"\"\"\n",
    "    def __init__(self, k_model, x_tr, y_tr, batch_size):\n",
    "        \"\"\"\n",
    "        assign model to class, calculate shape of weights, and arrays containing them (latter possibly redundant)\n",
    "        \"\"\"\n",
    "        self.weight_shapes = []\n",
    "        self.weights = [] #delete after testing\n",
    "        self.num_weights = 0 #delete after testing\n",
    "        self.oned_weights = np.array([]) #possibly delete after testing\n",
    "        self.model = k_model\n",
    "        self.x_tr = x_tr\n",
    "        self.y_tr = y_tr\n",
    "        self.m = x_tr.shape[0]                  \n",
    "        self.batch_size = batch_size\n",
    "        self.num_complete_batches = int(np.floor(float(self.m)/self.batch_size))\n",
    "        self.num_batches = int(np.ceil(float(self.m)/self.batch_size))\n",
    "        self.get_weight_info()\n",
    "        \n",
    "    def calc_gauss_LL(self, x, y, LL_var = 1.):\n",
    "        \"\"\"\n",
    "        WARNING: batch size given here should be same as one given in get_LL_const, or \n",
    "        normalisation constant won't be correct.\n",
    "        as above, only supports scalar variance.\n",
    "        \"\"\"\n",
    "        return - 1. / (2. * LL_var) * self.model.evaluate(x, y) + self.LL_const \n",
    "        \n",
    "    def setup_LL(self, LL_var = 1.):\n",
    "        \"\"\"\n",
    "        calculates LL constant, and sets correct LL function, creates generator object for batches.\n",
    "        currently only supports single (scalar) variance across all records and outputs,\n",
    "        since we use model.evaluate() to calculate cost with predefined loss functions (e.g. mse, crossentropy) \n",
    "        which evaluates sums across examples/outputs\n",
    "        (so variance can't be included in each summation).\n",
    "        if we instead calculate likelihood from final layer output, will probably use scipy.stats to do so\n",
    "        and this function will become redundant. \n",
    "        if instead we define own loss function which allows for different variances,\n",
    "        comment out lines below (and variance should be LL_dim x LL_dim array) and uncomment ones further down\n",
    "        \"\"\"\n",
    "        if self.m <= self.batch_size:\n",
    "            self.batch_generator = None\n",
    "        else:\n",
    "            self.batch_generator = self.create_batch_generator()\n",
    "        output_size = np.prod(np.shape(model.layers[-1].output.shape)) #np.prod is in case output isn't vector\n",
    "        LL_dim = self.batch_size * output_size\n",
    "        if self.model.loss == 'mse':\n",
    "            #temporary\n",
    "            self.LL_const = -0.5 * LL_dim * (np.log(2. * np.pi) + np.log(LL_var))\n",
    "            self.LL = self.calc_gauss_LL\n",
    "            #longer term solution (see comments above)\n",
    "            #self.LL_const = -0.5 * (LL_dim * np.log(2. * np.pi) + np.log(np.linalg.det(variance)))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "    def get_weight_info(self):\n",
    "        \"\"\"\n",
    "        weight arrays may be redundant (see above)\n",
    "        \"\"\"\n",
    "        for layer_weight in self.model.get_weights():\n",
    "            layer_shape = layer_weight.shape\n",
    "            self.weight_shapes.append(layer_shape)\n",
    "            self.weights.append(layer_weight) #delete after testing\n",
    "            self.oned_weights = np.concatenate((self.oned_weights, layer_weight.reshape(-1))) #possibly delete after testing\n",
    "            self.num_weights += np.prod(layer_shape) #delete after testing\n",
    "        \n",
    "    def get_weight_shapes(self):\n",
    "        return self.weight_shapes\n",
    "    \n",
    "    def get_weights(self): #delete after testing\n",
    "        return self.weights\n",
    "    \n",
    "    def get_oned_weights(self): #possibly delete after testing\n",
    "        return self.oned_weights\n",
    "    \n",
    "    def get_oned_weights2(self): #possibly delete after testing\n",
    "        \"\"\"\n",
    "        used in case where we want array of nn initial weights to pass to pc, but after that\n",
    "        it can be deleted.\n",
    "        WARNING: deletes self.oned_weights\n",
    "        \"\"\"\n",
    "        temp = self.oned_weights\n",
    "        del self.oned_weights\n",
    "        return temp\n",
    "    \n",
    "    def get_num_weights(self): #delete after testing\n",
    "        return self.num_weights\n",
    "    \n",
    "    def get_model(self):\n",
    "        \"\"\"\n",
    "        return keras model instance\n",
    "        \"\"\"\n",
    "        return self.model\n",
    "    \n",
    "    def get_model_summary(self):\n",
    "        \"\"\"\n",
    "        return keras model summary\n",
    "        \"\"\"\n",
    "        return self.model.summary()\n",
    "    \n",
    "    def get_model_weights(self):\n",
    "        \"\"\"\n",
    "        returns list of weight arrays (one element for each layer's set of weight/bias)\n",
    "        \"\"\"\n",
    "        return self.model.get_weights()\n",
    "    \n",
    "    def set_weights(self, weights): #delete after testing\n",
    "        self.weights = weights\n",
    "        \n",
    "    def set_oned_weights(self, oned_weights): #possibly delete after testing\n",
    "        self.oned_weights = oned_weights\n",
    "        \n",
    "    def set_model_weights(self, weights):\n",
    "        self.model.set_weights(weights)\n",
    "        \n",
    "    def set_k_weights(self, new_oned_weights):\n",
    "        \"\"\"\n",
    "        set weights of keras.Model using 1d array of weights.\n",
    "        beside this, updates weight array attributes (which may be deleted after testing).\n",
    "        \"\"\"\n",
    "        self.set_oned_weights(new_oned_weights) #possibly delete after testing\n",
    "        new_weights = []\n",
    "        start_index = 0\n",
    "        for weight_shape in self.get_weight_shapes():\n",
    "            weight_size = np.prod(weight_shape)\n",
    "            new_weights.append(new_oned_weights[start_index:start_index + weight_size].reshape(weight_shape))\n",
    "            start_index += weight_size\n",
    "        self.set_weights(new_weights) #delete after testing\n",
    "        self.set_model_weights(new_weights)\n",
    "        \n",
    "    def __call__(self, oned_weights):\n",
    "        \"\"\"\n",
    "        sets keras.Model weights, gets new batch of training data (or full batch), \n",
    "        evaluates log likelihood function and returns its value.\n",
    "        to be passed to polychord as loglikelihood function\n",
    "        \"\"\"\n",
    "        self.set_k_weights(oned_weights)\n",
    "        x_batch, y_batch = self.get_batch()\n",
    "        LL = self.LL(x_batch, y_batch)\n",
    "        return LL\n",
    "        \n",
    "    def get_batch(self):\n",
    "        \"\"\"\n",
    "        returns either entire training data, or uses batch generator object to generate\n",
    "        new batch.\n",
    "        \"\"\"\n",
    "        if self.m <= self.batch_size:\n",
    "            return self.x_tr, self.y_tr\n",
    "        else:\n",
    "            return self.batch_generator.next()\n",
    "            \n",
    "    def create_batch_generator(self):\n",
    "        \"\"\"\n",
    "        creates a batch generator object which yields a batch (subset) of the training data\n",
    "        from a list of batches created by create_batches(). in the case that the end of the list\n",
    "        is reached, calls create_batches to create a new list of batches and returns first in list.\n",
    "        \"\"\"\n",
    "        i = 0\n",
    "        batches = self.create_batches()\n",
    "        while True:\n",
    "            if i < self.num_batches:\n",
    "                pass #don't need to create new random shuffle of training data\n",
    "            else:\n",
    "                batches = self.create_batches()\n",
    "                i = 0\n",
    "            yield batches[i]\n",
    "            i += 1\n",
    "    \n",
    "    def create_batches(self):\n",
    "        \"\"\"\n",
    "        create batches of size self.batch_size from self.m training examples\n",
    "        for training.\n",
    "        In case of large training sets, batches may want to overwrite self.x_tr/self.y_tr\n",
    "        (if not both have to be saved to memory as batches is saved in generator object).\n",
    "        NOTE: in case of batch_size not being factor of m, last batch in list is of size\n",
    "        < batch_size, so likelihood calculation is INCORRECT (normalisation constant).\n",
    "        even if used correct normalisation constant, wouldn't be consistent with other calculations.\n",
    "        Thus for Bayesian problems we should probably just discard these \n",
    "        extra training examples and ensure batch_size / m is an integer.\n",
    "        \"\"\"\n",
    "        batches = []\n",
    "        # Step 1: Shuffle x, y\n",
    "        permutation = np.random.permutation(self.m)\n",
    "        shuffled_x = self.x_tr[permutation]\n",
    "        shuffled_y = self.y_tr[permutation]\n",
    "        # Step 2: Partition (shuffled_x, shuffled_y). Minus the end case.\n",
    "        # number of batches of size self.batch_size in your partitionning\n",
    "        for i in range(self.num_complete_batches):\n",
    "            batch_x = shuffled_x[self.batch_size * i: self.batch_size * (i + 1)]\n",
    "            batch_y = shuffled_y[self.batch_size * i: self.batch_size * (i + 1)]\n",
    "            batch = (batch_x, batch_y)\n",
    "            batches.append(batch)\n",
    "    \n",
    "        # Handling the end case (last batch < self.batch_size)\n",
    "        if self.num_complete_batches != self.num_batches:\n",
    "            batch_x = shuffled_x[self.num_complete_batches * self.batch_size:]\n",
    "            batch_y = shuffled_y[self.num_complete_batches * self.batch_size:]\n",
    "            batch = (batch_x, batch_y)\n",
    "            batches.append(batch)\n",
    "        return batches\n",
    "    \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
