{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.neural_network\n",
    "import numpy as np\n",
    "import metrics as mets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9e6b00c1a3d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../data/21_cm/8_params_21_2_x_tr.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../data/21_cm/8_params_21_2_y_tr.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#sklearn outputs y_pred as 1d so keep 1d here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# data\n",
    "x_train = np.genfromtxt(\"../../data/21_cm/8_params_21_2_x_tr.csv\", delimiter=\",\")\n",
    "y_train = np.genfromtxt(\"../../data/21_cm/8_params_21_2_y_tr.csv\", delimiter=\",\") #sklearn outputs y_pred as 1d so keep 1d here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#architecture parameters\n",
    "num_inputs = x_train.shape[1] #aka n in ml terminology\n",
    "num_outputs = y_train.shape[1] #dimensionality of output y (for single record)\n",
    "layer_sizes = [32, 64, 64, 32]\n",
    "#propagation parameters\n",
    "epochs = 1000\n",
    "m = x_train.shape[0] #total number of records\n",
    "batch_num = m / 2 #with sklearn, get MemoryError if use full batch at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.neural_network.MLPRegressor(hidden_layer_sizes=layer_sizes, activation='tanh', solver='adam', alpha=0.000, batch_size=batch_num, learning_rate='constant', learning_rate_init=0.01, power_t=0.5, max_iter=epochs, shuffle=True, random_state=None, tol=0.0001, verbose=True, warm_start=True, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0., beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = -50.35104929 \n",
    "var = 5410.76986054\n",
    "n_z = 136"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.38327348\n",
      "Iteration 2, loss = 0.64121793\n",
      "Iteration 3, loss = 0.66702792\n",
      "Iteration 4, loss = 0.50523676\n",
      "Iteration 5, loss = 0.53379367\n",
      "Iteration 6, loss = 0.47430718\n",
      "Iteration 7, loss = 0.49322640\n",
      "Iteration 8, loss = 0.48193324\n",
      "Iteration 9, loss = 0.44875421\n",
      "Iteration 10, loss = 0.44587218\n",
      "Iteration 11, loss = 0.43121299\n",
      "Iteration 12, loss = 0.41652282\n",
      "Iteration 13, loss = 0.39869045\n",
      "Iteration 14, loss = 0.37750083\n",
      "Iteration 15, loss = 0.36117508\n",
      "Iteration 16, loss = 0.33819463\n",
      "Iteration 17, loss = 0.32493818\n",
      "Iteration 18, loss = 0.31102952\n",
      "Iteration 19, loss = 0.30397148\n",
      "Iteration 20, loss = 0.30264049\n",
      "Iteration 21, loss = 0.29696242\n",
      "Iteration 22, loss = 0.29646987\n",
      "Iteration 23, loss = 0.29241957\n",
      "Iteration 24, loss = 0.28798324\n",
      "Iteration 25, loss = 0.28443310\n",
      "Iteration 26, loss = 0.27948638\n",
      "Iteration 27, loss = 0.27555914\n",
      "Iteration 28, loss = 0.27099922\n",
      "Iteration 29, loss = 0.26768708\n",
      "Iteration 30, loss = 0.26407394\n",
      "Iteration 31, loss = 0.26148162\n",
      "Iteration 32, loss = 0.25825200\n",
      "Iteration 33, loss = 0.25512937\n",
      "Iteration 34, loss = 0.25188724\n",
      "Iteration 35, loss = 0.24896818\n",
      "Iteration 36, loss = 0.24624035\n",
      "Iteration 37, loss = 0.24383095\n",
      "Iteration 38, loss = 0.24142863\n",
      "Iteration 39, loss = 0.23905325\n",
      "Iteration 40, loss = 0.23679704\n",
      "Iteration 41, loss = 0.23471425\n",
      "Iteration 42, loss = 0.23260312\n",
      "Iteration 43, loss = 0.23043973\n",
      "Iteration 44, loss = 0.22825930\n",
      "Iteration 45, loss = 0.22602009\n",
      "Iteration 46, loss = 0.22372595\n",
      "Iteration 47, loss = 0.22161445\n",
      "Iteration 48, loss = 0.22411414\n",
      "Iteration 49, loss = 0.24008764\n",
      "Iteration 50, loss = 0.21905018\n",
      "Iteration 51, loss = 0.22126554\n",
      "Iteration 52, loss = 0.21779088\n",
      "Iteration 53, loss = 0.21177179\n",
      "Iteration 54, loss = 0.21409440\n",
      "Iteration 55, loss = 0.20899787\n",
      "Iteration 56, loss = 0.20851134\n",
      "Iteration 57, loss = 0.20695092\n",
      "Iteration 58, loss = 0.20364532\n",
      "Iteration 59, loss = 0.20024083\n",
      "Iteration 60, loss = 0.20309996\n",
      "Iteration 61, loss = 0.20427570\n",
      "Iteration 62, loss = 0.26661883\n",
      "Iteration 63, loss = 0.22759339\n",
      "Iteration 64, loss = 0.25102876\n",
      "Iteration 65, loss = 0.23123479\n",
      "Iteration 66, loss = 0.22795627\n",
      "Iteration 67, loss = 0.21720562\n",
      "Iteration 68, loss = 0.21054522\n",
      "Iteration 69, loss = 0.20823422\n",
      "Iteration 70, loss = 0.20757908\n",
      "Iteration 71, loss = 0.20622930\n",
      "Iteration 72, loss = 0.20266389\n",
      "Iteration 73, loss = 0.19900446\n",
      "Iteration 74, loss = 0.19623641\n",
      "Iteration 75, loss = 0.19384755\n",
      "Iteration 76, loss = 0.19174548\n",
      "Iteration 77, loss = 0.18977613\n",
      "Iteration 78, loss = 0.18783192\n",
      "Iteration 79, loss = 0.18581201\n",
      "Iteration 80, loss = 0.18394380\n",
      "Iteration 81, loss = 0.18202102\n",
      "Iteration 82, loss = 0.18109743\n",
      "Iteration 83, loss = 0.18077894\n",
      "Iteration 84, loss = 0.17794319\n",
      "Iteration 85, loss = 0.17797949\n",
      "Iteration 86, loss = 0.17565750\n",
      "Iteration 87, loss = 0.17652075\n",
      "Iteration 88, loss = 0.18044022\n",
      "Iteration 89, loss = 0.22627451\n",
      "Iteration 90, loss = 0.22985678\n",
      "Iteration 91, loss = 0.25727753\n",
      "Iteration 92, loss = 0.22924594\n",
      "Iteration 93, loss = 0.22661630\n",
      "Iteration 94, loss = 0.21585698\n",
      "Iteration 95, loss = 0.20027228\n",
      "Iteration 96, loss = 0.20273951\n",
      "Iteration 97, loss = 0.20421377\n",
      "Iteration 98, loss = 0.19498022\n",
      "Iteration 99, loss = 0.18879215\n",
      "Iteration 100, loss = 0.18849514\n",
      "Iteration 101, loss = 0.18704066\n",
      "Iteration 102, loss = 0.18390614\n",
      "Iteration 103, loss = 0.18075303\n",
      "Iteration 104, loss = 0.17840181\n",
      "Iteration 105, loss = 0.17701064\n",
      "Iteration 106, loss = 0.17571753\n",
      "Iteration 107, loss = 0.17425061\n",
      "Iteration 108, loss = 0.17196368\n",
      "Iteration 109, loss = 0.17061481\n",
      "Iteration 110, loss = 0.16921590\n",
      "Iteration 111, loss = 0.16783366\n",
      "Iteration 112, loss = 0.16708853\n",
      "Iteration 113, loss = 0.16569125\n",
      "Iteration 114, loss = 0.16373933\n",
      "Iteration 115, loss = 0.16375551\n",
      "Iteration 116, loss = 0.16250484\n",
      "Iteration 117, loss = 0.16406152\n",
      "Iteration 118, loss = 0.18989264\n",
      "Iteration 119, loss = 0.20579189\n",
      "Iteration 120, loss = 0.20618742\n",
      "Iteration 121, loss = 0.21527282\n",
      "Iteration 122, loss = 0.19680761\n",
      "Iteration 123, loss = 0.18625958\n",
      "Iteration 124, loss = 0.18303101\n",
      "Iteration 125, loss = 0.17491331\n",
      "Iteration 126, loss = 0.17367271\n",
      "Iteration 127, loss = 0.17166607\n",
      "Iteration 128, loss = 0.16899689\n",
      "Iteration 129, loss = 0.16609572\n",
      "Iteration 130, loss = 0.16451869\n",
      "Iteration 131, loss = 0.16327781\n",
      "Iteration 132, loss = 0.16168055\n",
      "Iteration 133, loss = 0.16031340\n",
      "Iteration 134, loss = 0.15894876\n",
      "Iteration 135, loss = 0.15724929\n",
      "Iteration 136, loss = 0.15630787\n",
      "Iteration 137, loss = 0.15488917\n",
      "Iteration 138, loss = 0.15351912\n",
      "Iteration 139, loss = 0.15450257\n",
      "Iteration 140, loss = 0.19335514\n",
      "Iteration 141, loss = 0.33348808\n",
      "Iteration 142, loss = 0.25340706\n",
      "Iteration 143, loss = 0.26426913\n",
      "Iteration 144, loss = 0.24081707\n",
      "Iteration 145, loss = 0.22104925\n",
      "Iteration 146, loss = 0.21866707\n",
      "Iteration 147, loss = 0.21086466\n",
      "Iteration 148, loss = 0.20448867\n",
      "Iteration 149, loss = 0.19842474\n",
      "Iteration 150, loss = 0.19164260\n",
      "Iteration 151, loss = 0.18480260\n",
      "Iteration 152, loss = 0.18414756\n",
      "Iteration 153, loss = 0.17927297\n",
      "Iteration 154, loss = 0.17712648\n",
      "Iteration 155, loss = 0.17282897\n",
      "Iteration 156, loss = 0.17034689\n",
      "Iteration 157, loss = 0.16803256\n",
      "Iteration 158, loss = 0.16506942\n",
      "Iteration 159, loss = 0.16339210\n",
      "Iteration 160, loss = 0.16134981\n",
      "Iteration 161, loss = 0.15889201\n",
      "Iteration 162, loss = 0.15756904\n",
      "Iteration 163, loss = 0.15618595\n",
      "Iteration 164, loss = 0.15508990\n",
      "Iteration 165, loss = 0.15222983\n",
      "Iteration 166, loss = 0.15088278\n",
      "Iteration 167, loss = 0.15356657\n",
      "Iteration 168, loss = 0.25940982\n",
      "Iteration 169, loss = 0.27481516\n",
      "Iteration 170, loss = 0.32738156\n",
      "Iteration 171, loss = 0.29913490\n",
      "Iteration 172, loss = 0.27760464\n",
      "Iteration 173, loss = 0.26294211\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-74c0e14e5e25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/kamran/anaconda2/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mMLP\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m         \"\"\"\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kamran/anaconda2/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, incremental)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_STOCHASTIC_SOLVERS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             self._fit_stochastic(X, y, activations, deltas, coef_grads,\n\u001b[0;32m--> 371\u001b[0;31m                                  intercept_grads, layer_units, incremental)\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0;31m# Run the LBFGS solver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kamran/anaconda2/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.pyc\u001b[0m in \u001b[0;36m_fit_stochastic\u001b[0;34m(self, X, y, activations, deltas, coef_grads, intercept_grads, layer_units, incremental)\u001b[0m\n\u001b[1;32m    510\u001b[0m                     batch_loss, coef_grads, intercept_grads = self._backprop(\n\u001b[1;32m    511\u001b[0m                         \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_slice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_slice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m                         coef_grads, intercept_grads)\n\u001b[0m\u001b[1;32m    513\u001b[0m                     accumulated_loss += batch_loss * (batch_slice.stop -\n\u001b[1;32m    514\u001b[0m                                                       batch_slice.start)\n",
      "\u001b[0;32m/home/kamran/anaconda2/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.pyc\u001b[0m in \u001b[0;36m_backprop\u001b[0;34m(self, X, y, activations, deltas, coef_grads, intercept_grads)\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mdeltas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeltas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoefs_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0minplace_derivative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDERIVATIVES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m             \u001b[0minplace_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             coef_grads, intercept_grads = self._compute_loss_grad(\n",
      "\u001b[0;32m/home/kamran/anaconda2/lib/python2.7/site-packages/sklearn/neural_network/_base.pyc\u001b[0m in \u001b[0;36minplace_tanh_derivative\u001b[0;34m(Z, delta)\u001b[0m\n\u001b[1;32m    149\u001b[0m          \u001b[0mThe\u001b[0m \u001b[0mbackpropagated\u001b[0m \u001b[0merror\u001b[0m \u001b[0msignal\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mmodified\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \"\"\"\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mdelta\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mZ\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train model\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7d31fe310e02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "print y_train.mean()\n",
    "y_pred = np.zeros_like(y_train)\n",
    "y_pred[:m / 2] = model.predict(x_train[:m / 2])\n",
    "y_pred[m / 2:] = model.predict(x_train[m / 2:])\n",
    "print y_pred.mean()\n",
    "print y_train.var()\n",
    "print y_pred.var()\n",
    "print 'rmse'\n",
    "print mets.twenty_one_cm_rmse_higher_order(mean, var)(y_train, y_pred)\n",
    "print 'rmse ts mean'\n",
    "print mets.twenty_one_cm_rmse_ts_mean_higher_order(y_train, y_pred, mean, var, n_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
